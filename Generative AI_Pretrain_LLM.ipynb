{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300c6b06-ea3f-45c6-a02c-846e1ca6f8d5",
   "metadata": {},
   "source": [
    "# Generative AI Assignment – Using `google/flan-t5-small`\n",
    "\n",
    "This notebook demonstrates how to use a pre-trained Large Language Model (LLM) from Hugging Face to perform text generation using prompt engineering. The model used is `google/flan-t5-small`, chosen for its lightweight architecture and versatility across tasks.\n",
    "\n",
    "### Goals:\n",
    "- Load and run a pre-trained LLM\n",
    "- Apply prompt engineering to guide output\n",
    "- Experiment with key generation parameters (temperature, max tokens, top-p)\n",
    "- Analyze how parameter changes affect output quality and style\n",
    "\n",
    "This notebook supports the Fall 2025 assignment on Generative AI and Pre-trained LLMs, and is designed for modular clarity and rubric alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4c23d-3411-46d8-ac7b-46df565213c3",
   "metadata": {},
   "source": [
    "##### Package Installation\n",
    "This step installs the required Python libraries for working with pre‑trained Large Language Models (LLMs):\n",
    "\n",
    "- **transformers**: Hugging Face’s library that provides access to pre‑trained models, tokenizers, and pipelines for tasks like text generation, summarization, and translation.  \n",
    "- **torch**: PyTorch, the deep learning framework used as the backend for running and training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b22c97a-eb59-43bd-85c6-98610e00c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\barrc\\anaconda3\\envs\\generativeai_withpretrained_llm\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10216eb4-bcf6-4da2-bc27-d0c5d0846037",
   "metadata": {},
   "source": [
    "#### 1. Model Setup\n",
    "\n",
    "This section loads the `google/flan-t5-small` model and tokenizer from Hugging Face. This model supports a variety of tasks including text generation, summarization, and question answering. It is lightweight and ideal for prompt engineering experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36f345-a5db-45a9-89dc-8f114040f344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac4e577-3880-4371-aadf-18f47b83b9b6",
   "metadata": {},
   "source": [
    "##### Environment Configuration\n",
    "This cell sets an environment variable to suppress Hugging Face Hub symlink warnings:  \n",
    "```python\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287e6895-4dfd-4409-8559-835556b4e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd0331-71d6-4cf9-b8f5-84b3bcb6ac22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1f70ae-1253-47e7-a442-7e6e1fe468ad",
   "metadata": {},
   "source": [
    "##### Model Setup: Loading Flan‑T5\n",
    "In this section we initialize the pre‑trained **Flan‑T5 Small** model from Hugging Face.  \n",
    "- **AutoTokenizer**: Converts human text into token IDs the model can process, and back into text.  \n",
    "- **AutoModelForSeq2SeqLM**: Loads the sequence‑to‑sequence model weights, enabling tasks like translation, summarization, and Q&A.  \n",
    "- **Model name**: `\"google/flan-t5-small\"` is chosen for its lightweight architecture, making it ideal for experimentation with prompt engineering.  \n",
    "\n",
    "This setup step ensures that later cells can run prompts through the model for text generation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8bebfd-2d74-4f65-8d5d-7e219355e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define model name\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc3e11-efa2-4f5a-8a90-3d69725a2f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7668fe9-283e-4b37-bc8e-109ba268660a",
   "metadata": {},
   "source": [
    "##### Quick Demo: Hugging Face Pipeline\n",
    "This cell uses the high-level `pipeline(\"text2text-generation\")` API with Flan‑T5.  \n",
    "It asks the question *\"What is the capital of France?\"* and prints the generated output.  \n",
    "\n",
    "⚠️ Note: The pipeline sometimes produces incorrect or echoed answers (e.g., \"France\" instead of \"Paris\") because the prompt is underspecified.  \n",
    "This demonstrates the importance of **prompt engineering** — adding explicit task framing like  \n",
    "*\"Question: What is the capital of France? Answer:\"* yields more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf190d55-e374-4fbe-a27b-48f5bf1a4368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'france'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = generator(\"What is the capital of France?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322057e-797b-44be-a360-d387c90fdbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7b2aa24-cf9a-47e0-8c97-9d7e408837c6",
   "metadata": {},
   "source": [
    "##### Test Case 1: Baseline Prompt (No Parameters)\n",
    "This cell runs the first prompt through the Flan‑T5 model using the default `generate()` settings.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameters:** None explicitly set — the model uses greedy decoding by default.  \n",
    "- **Purpose:** Serves as the baseline output for comparison against later test cases where parameters like `temperature` and `top_p` are varied.  \n",
    "This allows us to see how the model behaves deterministically before introducing randomness or sampling diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b699dbc-ab15-4e69-b29a-295f7f1a5a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La météo est très agréable aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate output with more tokens allowed\n",
    "outputs = model.generate(input_ids, max_new_tokens=20)\n",
    "\n",
    "# Decode and print result\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162610b1-04df-4503-97fe-9e9da51cb399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60cb7b8-2a95-4ab1-a63b-bdc0f0326e89",
   "metadata": {},
   "source": [
    "##### Test Case 2: Varying Temperature\n",
    "This run modifies only the `temperature` parameter to introduce controlled randomness.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameter change:** `temperature=0.7` (moderate randomness)  \n",
    "- **Purpose:** Compare against the baseline (Cell 5) to see how temperature affects phrasing and creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb83d9b7-91f8-4ff7-bc4a-b1bb297c280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'atmosphère est très nice today.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,       # keep length consistent\n",
    "    temperature=0.7,     # only parameter changed\n",
    "    do_sample=True       # required for temperature to take effect\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6ad41-b22b-452d-8882-800e343ae23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a046b89c-45cb-479f-98ec-4239b9f88bac",
   "metadata": {},
   "source": [
    "##### Test Case 3: High Temperature\n",
    "This run modifies only the `temperature` parameter to a higher value.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameter change:** `temperature=1.0` (high randomness)  \n",
    "- **Purpose:** Compare against the baseline (Cell 5) and moderate run (Cell 7) to see how increased randomness affects phrasing, creativity, and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62a241a-bb1d-4ffe-bee4-1e8bbd7b8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is nice today.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,       # keep length consistent\n",
    "    temperature=1.0,     # higher randomness\n",
    "    do_sample=True       # required for temperature to take effect\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8bb465-88bd-4f8e-a584-e04e95319faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd564c3b-9a5e-4746-acd6-bb1ca3e425dd",
   "metadata": {},
   "source": [
    "##### Test Case 4: Top‑p Sampling (Conservative)\n",
    "This run modifies only the `top_p` parameter to restrict sampling diversity.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameter change:** `top_p=0.7` (conservative sampling, fewer candidate tokens)  \n",
    "- **Purpose:** Compare against baseline (Cell 5) to see how limiting sampling affects phrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c453a2-6cec-4798-81f0-7b167c57c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La météo est agréable aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    top_p=0.7,        # only parameter changed\n",
    "    do_sample=True    # required for top-p to take effect\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b9220-0b64-453c-b715-b834ae230613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786bd7f2-34f2-4ede-80e0-401c33b5d4fc",
   "metadata": {},
   "source": [
    "##### Test Case 5: Top‑p Sampling (Balanced)\n",
    "This run modifies only the `top_p` parameter to allow moderate diversity in token selection.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameter change:** `top_p=0.9` (balanced sampling, broader candidate pool than conservative run)  \n",
    "- **Purpose:** Compare against the conservative run (Cell 11) to see how increasing diversity changes phrasing while maintaining coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc5420b-8304-4ae5-8810-09247b49f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La température serait légèrement proche.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    top_p=0.9,        # only parameter changed\n",
    "    do_sample=True    # required for top-p to take effect\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1f99d-3e25-443e-b077-46460587c261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70402cfd-0d5b-424c-8bc5-8df1d9f294d5",
   "metadata": {},
   "source": [
    "##### Test Case 6: Top‑p Sampling (Very Open)\n",
    "This run modifies only the `top_p` parameter to allow maximum diversity in token selection.  \n",
    "- **Prompt:** `\"Translate English to French: The weather is nice today.\"`  \n",
    "- **Parameter change:** `top_p=1.0` (all candidate tokens considered)  \n",
    "- **Purpose:** Compare against Cells 11 (conservative) and 13 (balanced) to see how maximum diversity affects phrasing, creativity, and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f389e8c4-ced2-4e38-ae67-993ef7971412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps est très bien.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    top_p=1.0,        # maximum diversity\n",
    "    do_sample=True    # required for top-p to take effect\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78676722-cf1f-40fe-9f78-398764974838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
